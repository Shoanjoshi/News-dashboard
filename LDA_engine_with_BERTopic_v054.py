# ============================================
# LDA_engine_with_BERTopic_v_fixed_clusters.py
# Stable engine with:
#  - BERTopic clustering using fixed clusters via KMeans
#  - GPT summaries with bullet logic
#  - Expanded RSS feeds (NYT, WaPo, Cybersecurity sources)
# ============================================

import os
import feedparser
import numpy as np
from collections import Counter

from openai import OpenAI
from bertopic import BERTopic
from sklearn.cluster import KMeans
from umap import UMAP
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# --------------------------------------------
# OpenAI client
# --------------------------------------------
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# --------------------------------------------
# Expanded RSS feeds
# --------------------------------------------
RSS_FEEDS = [
    # Existing major feeds
    "https://feeds.reuters.com/reuters/businessNews",
    "https://feeds.reuters.com/reuters/markets",
    "https://www.ft.com/rss/home/us",
    "https://www.wsj.com/xml/rss/3_7014.xml",
    "https://www.wsj.com/xml/rss/3_7085.xml",
    "https://feeds.marketwatch.com/marketwatch/topstories/",
    "https://feeds.marketwatch.com/marketwatch/marketpulse/",
    "http://feeds.bbci.co.uk/news/business/rss.xml",
    "http://rss.cnn.com/rss/edition_business.rss",

    # International
    "https://www.ft.com/rss/home/europe",
    "https://www.ft.com/rss/home/asia",
    "https://asia.nikkei.com/rss/feed",
    "https://www.scmp.com/rss/91/feed",

    # Technology
    "https://feeds.reuters.com/reuters/technologyNews",
    "https://feeds.feedburner.com/TechCrunch/",

    # Macro + Risk + Regulation
    "https://www.ft.com/rss/home",
    "https://feeds.a.dj.com/rss/RSSMarketsMain.xml",
    "https://www.investing.com/rss/news_25.rss",
    "https://www.investing.com/rss/news_1.rss",
    "https://www.investing.com/rss/news_285.rss",
    "https://www.federalreserve.gov/feeds/data.xml",
    "https://www.federalreserve.gov/feeds/press_all.xml",
    "https://markets.businessinsider.com/rss",
    "https://www.risk.net/feeds/rss",
    "https://www.forbes.com/finance/feed",
    "https://feeds.feedburner.com/alternativeinvestmentnews",
    "https://www.eba.europa.eu/eba-news-rss",
    "https://www.bis.org/rss/press_rss.xml",
    "https://www.imf.org/external/np/exr/feeds/rss.aspx?type=imfnews",

    # NEW FEED ADDITIONS FOR VARIETY

    ## Major US newspapers
    "https://rss.nytimes.com/services/xml/rss/nyt/Economy.xml",
    "https://rss.nytimes.com/services/xml/rss/nyt/Business.xml",
    "https://feeds.washingtonpost.com/rss/business",
    "https://feeds.washingtonpost.com/rss/business/economy",

    ## Cybersecurity feeds
    "https://krebsonsecurity.com/feed/",
    "https://www.bleepingcomputer.com/feed/",
    "https://www.darkreading.com/rss.xml",
    "https://www.scmagazine.com/section/feed",

    ## Technology + risk
    "https://feeds.arstechnica.com/arstechnica/technology-lab",
]

# --------------------------------------------
# Themes (unchanged here)
# --------------------------------------------
THEME_DESCRIPTIONS = {
    "Recessionary pressures": "Economic slowdown, declining demand, unemployment risk, or business contraction.",
    "Inflation": "Persistent price increases and monetary policy response impacting costs and purchasing power.",
    "Private credit": "Non-bank lending, private debt fund activity, liquidity constraints, and leveraged finance risk.",
    "AI": "Artificial intelligence development, enterprise adoption, automation, and regulatory concerns.",
    "Cyber attacks": "Cybersecurity breaches, systemic risks related to data or technology vulnerabilities.",
    "Commercial real estate": "Trends in office, retail, industrial, and hospitality real estate with refinancing risk.",
    "Consumer debt": "Household financial pressure, debt levels, delinquencies, affordability shifts.",
    "Bank lending and credit risk": "Credit exposure in banking portfolios, regulatory pressure, and default risks.",
    "Others": "Articles not directly linked to financial systemic themes.",
}

SIMILARITY_THRESHOLD = 0.20
# --------------------------------------------
# Helper normalization
# --------------------------------------------
def _normalize_rows(mat: np.ndarray) -> np.ndarray:
    norms = np.linalg.norm(mat, axis=1, keepdims=True)
    norms[norms == 0] = 1.0
    return mat / norms

# --------------------------------------------
# Fetch articles
# --------------------------------------------
def fetch_articles():
    docs = []
    for feed in RSS_FEEDS:
        try:
            parsed = feedparser.parse(feed)
            for entry in parsed.entries[:20]:
                content = (
                    entry.get("summary")
                    or entry.get("description")
                    or entry.get("title")
                    or ""
                )
                if isinstance(content, str):
                    clean = content.strip()
                    if len(clean) > 50:
                        docs.append(clean[:1200])
        except Exception as e:
            print(f"âš  Feed error {feed}: {e}")

    print(f"ðŸ”Ž RSS articles extracted: {len(docs)}")
    if docs:
        print(f"ðŸ“Œ Sample article: {docs[0][:120]}")
    return docs

# --------------------------------------------
# GPT topic summarizer
# --------------------------------------------
def gpt_summarize_topic(topic_id, docs_for_topic):
    """
    Improved topic summarization using representative docs with holistic summary.
    Ensures summaries match topic map keywords and removes bullet formatting.
    """

    MAX_SUMMARY_DOCS = 8  # use more docs for better representation
    docs_selected = docs_for_topic[:MAX_SUMMARY_DOCS]

    # Clear separation between documents
    text = "\n\n".join([f"ARTICLE:\n{doc}" for doc in docs_selected])

    prompt = f"""
You are preparing an objective briefing based ONLY on the content provided.
Summarize the central theme of the topic without referencing individual articles.
Avoid subjective tone, speculation, or predictions. Stick to facts.

STRICT FORMAT:

TITLE: <3â€“5 WORDS, UPPERCASE, factual>
SUMMARY: <2â€“4 concise sentences that capture the main theme. No bullet points. No article separation.>

Content to analyze:
{text}
"""

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
        )
        out = resp.choices[0].message.content or ""

        if "TITLE:" in out:
            parts = out.split("TITLE:", 1)[1].split("SUMMARY:", 1)
            title = parts[0].strip()
            summary_text = parts[1].strip()

            summary_formatted = summary_text.replace("\n", " ")  # single paragraph
        else:
            title = f"TOPIC {topic_id}"
            summary_formatted = "Summary format incorrect."

        return {"title": title, "summary": summary_formatted}

    except Exception as e:
        print(f"âš  GPT error on topic {topic_id}: {e}")
        return {
            "title": f"TOPIC {topic_id}",
            "summary": "Summary generation failed.",
        }

# --------------------------------------------
# BERTopic model (fixed clusters)
# --------------------------------------------
def run_bertopic_analysis(docs):
    umap_model = UMAP(
        n_neighbors=30,
        n_components=2,
        min_dist=0.0,
        metric="cosine",
        random_state=42,
    )

    kmeans_model = KMeans(
        n_clusters=15,
        random_state=42,
        n_init="auto",
    )

    vectorizer_model = CountVectorizer(
        stop_words="english",
        max_df=1.0,
        min_df=2,
        ngram_range=(1, 3),
    )

    topic_model = BERTopic(
        umap_model=umap_model,
        hdbscan_model=kmeans_model,  # Forces fixed number of clusters
        vectorizer_model=vectorizer_model,
        calculate_probabilities=True,
        verbose=False,
    )

    topics, probs = topic_model.fit_transform(docs)
    return topic_model, topics, probs

# --------------------------------------------
# Main engine
# --------------------------------------------
def generate_topic_results():
    docs = fetch_articles()
    if not docs:
        return [], {}, None, {}, {}

    topic_model, topics, probs = run_bertopic_analysis(docs)
    topic_info = topic_model.get_topic_info()

    valid_topic_ids = [t for t in topic_info.Topic if t != -1]
    summaries, topic_embeddings = {}, {}

    if len(valid_topic_ids) < 5:
        print("âš  Only {} topics detected â€” activating fallback segmentation.".format(len(valid_topic_ids)))

    for topic_id in valid_topic_ids:
        doc_indices = [i for i, t in enumerate(topics) if t == topic_id]
        if not doc_indices:
            continue
        topic_docs = [docs[i] for i in doc_indices[:5]]
        summaries[topic_id] = gpt_summarize_topic(topic_id, topic_docs)
        topic_embeddings[topic_id] = topic_model.topic_embeddings_[topic_id].tolist()

    try:
        embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        article_embeddings = embedding_model.encode(
            docs, show_progress_bar=False
        )
        article_embeddings = _normalize_rows(np.array(article_embeddings))    
        # ðŸ†• Concatenate theme name with description before embedding
        theme_texts = [
            f"{theme}. {THEME_DESCRIPTIONS.get(theme, '')}"
            for theme in THEMES
        ]
        theme_texts.append("Others. General or unrelated articles not clearly tied to defined financial risk themes.")    
        theme_embeddings = embedding_model.encode(
            theme_texts, show_progress_bar=False
        )
    theme_embeddings = _normalize_rows(np.array(theme_embeddings))
    except Exception as e:
        print(f"âš  Embedding-based theme assignment failed: {e}")
        theme_metrics = {t: {"volume": 0, "centrality": 0.0} for t in THEMES}
        theme_metrics["Others"] = {"volume": len(docs), "centrality": 0.0}
        return docs, summaries, topic_model, topic_embeddings, theme_metrics

    theme_metrics = {t: {"volume": 0, "centrality": 0.0} for t in THEMES}
    theme_metrics["Others"] = {"volume": 0, "centrality": 0.0}

    # Multi-theme assignment + centrality overlap tracking
    theme_metrics = {
        theme: {"volume": 0, "centrality": 0.0, "articles": set()}
        for theme in THEMES
    }
    theme_metrics["Others"] = {"volume": 0, "centrality": 0.0, "articles": set()}
    
    article_theme_map = []  # Store the assigned themes per article for overlap logic
    
    for i, emb in enumerate(article_embeddings):
        sims = cosine_similarity([emb], theme_embeddings)[0]
        
        # Assign all themes above threshold
        assigned_themes = [
            THEMES[idx]
            for idx, score in enumerate(sims)
            if score >= SIMILARITY_THRESHOLD
        ]
    
        if not assigned_themes:
            assigned_themes = ["Others"]
    
        article_theme_map.append(assigned_themes)
    
        # Update topicality (volume) and track mentions
        for theme in assigned_themes:
            theme_metrics[theme]["volume"] += 1
            theme_metrics[theme]["articles"].add(i)
    
    # ---- Calculate centrality (theme overlap logic) ----
    for theme in THEMES:  # Skip "Others"
        overlaps = 0
        theme_articles = theme_metrics[theme]["articles"]
        
        for other_theme in THEMES:
            if other_theme != theme:
                other_articles = theme_metrics[other_theme]["articles"]
                overlaps += len(theme_articles.intersection(other_articles))
        
        theme_metrics[theme]["centrality_raw"] = overlaps
    
    # Normalize centrality (0â€“1 scale)
    max_overlap = max([theme_metrics[t]["centrality_raw"] for t in THEMES]) or 1
    for theme in THEMES:
        theme_metrics[theme]["centrality"] = theme_metrics[theme]["centrality_raw"] / max_overlap
    
    # Clear helper fields
    for t in theme_metrics:
        theme_metrics[t].pop("articles", None)
        theme_metrics[t].pop("centrality_raw", None)

    print(f"ðŸ“Š Theme metrics (volume only): {theme_metrics}")
    print("\n=== DEBUG: Topic Distribution ===\n", Counter(topics))

    return docs, summaries, topic_model, topic_embeddings, theme_metrics

# --------------------------------------------
# Local debug
# --------------------------------------------
if __name__ == "__main__":
    d, s, m, e, tm = generate_topic_results()
    print(f"Docs: {len(d)}, topics: {len(s)}")
    print("Themes:", tm)





